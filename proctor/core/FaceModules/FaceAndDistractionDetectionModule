import cv2
import mediapipe as mp
import numpy as np
import math


class DistractionDetector:
    def __init__(self):
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles

        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

        self.NOSE_TIP = 1
        self.CHIN = 152
        self.LEFT_EYE_CORNER = 226
        self.RIGHT_EYE_CORNER = 446
        self.LEFT_MOUTH_CORNER = 57
        self.RIGHT_MOUTH_CORNER = 287
        self.FACE_CENTER = 9
        self.LEFT_CHEEK = 234
        self.RIGHT_CHEEK = 454

        self.YAW_THRESHOLD = 25
        self.PITCH_THRESHOLD = 20
        self.ROLL_THRESHOLD = 25
        self.FACE_CENTER_THRESHOLD = 0.15

        self.distraction_counter = 0
        self.total_frames = 0
        self.consecutive_distracted_frames = 0
        self.DISTRACTION_CONFIRMATION_FRAMES = 5

    def get_face_center_position(self, landmarks, img_shape):
        h, w = img_shape[:2]
        face_center_x = landmarks[self.FACE_CENTER].x * w
        face_center_y = landmarks[self.FACE_CENTER].y * h
        screen_center_x = w / 2
        screen_center_y = h / 2
        x_dev = abs(face_center_x - screen_center_x) / w
        y_dev = abs(face_center_y - screen_center_y) / h
        return x_dev, y_dev, (face_center_x, face_center_y)

    def calculate_head_pose(self, landmarks, img_shape):
        h, w = img_shape[:2]
        model_points = np.array([
            (0.0, 0.0, 0.0),  # Nose
            (0.0, -330.0, -65.0),  # Chin
            (-225.0, 170.0, -135.0),  # Left eye
            (225.0, 170.0, -135.0),  # Right eye
            (-150.0, -150.0, -125.0),  # Left mouth
            (150.0, -150.0, -125.0)  # Right mouth
        ])
        image_points = np.array([
            (landmarks[self.NOSE_TIP].x * w, landmarks[self.NOSE_TIP].y * h),
            (landmarks[self.CHIN].x * w, landmarks[self.CHIN].y * h),
            (landmarks[self.LEFT_EYE_CORNER].x * w, landmarks[self.LEFT_EYE_CORNER].y * h),
            (landmarks[self.RIGHT_EYE_CORNER].x * w, landmarks[self.RIGHT_EYE_CORNER].y * h),
            (landmarks[self.LEFT_MOUTH_CORNER].x * w, landmarks[self.LEFT_MOUTH_CORNER].y * h),
            (landmarks[self.RIGHT_MOUTH_CORNER].x * w, landmarks[self.RIGHT_MOUTH_CORNER].y * h)
        ], dtype="double")
        camera_matrix = np.array([
            [w, 0, w / 2],
            [0, w, h / 2],
            [0, 0, 1]
        ], dtype="double")
        dist_coeffs = np.zeros((4, 1))

        try:
            success, rotation_vector, _ = cv2.solvePnP(
                model_points, image_points, camera_matrix, dist_coeffs
            )
            if not success:
                return 0, 0, 0, False
            rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
            sy = math.sqrt(rotation_matrix[0, 0] ** 2 + rotation_matrix[1, 0] ** 2)
            singular = sy < 1e-6
            if not singular:
                pitch = math.atan2(rotation_matrix[2, 1], rotation_matrix[2, 2])
                yaw = math.atan2(-rotation_matrix[2, 0], sy)
                roll = math.atan2(rotation_matrix[1, 0], rotation_matrix[0, 0])
            else:
                pitch = math.atan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])
                yaw = math.atan2(-rotation_matrix[2, 0], sy)
                roll = 0
            return math.degrees(pitch), math.degrees(yaw), math.degrees(roll), True
        except:
            return 0, 0, 0, False

    def detect(self, frame):
        rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_image)
        is_distracted = False
        face_detected = False
        reasons = []
        h, w = frame.shape[:2]

        if results.multi_face_landmarks:
            face_detected = True
            for landmarks_obj in results.multi_face_landmarks:
                landmarks = landmarks_obj.landmark

                pitch, yaw, roll, ok = self.calculate_head_pose(landmarks, frame.shape)

                if ok:
                    if abs(yaw) > self.YAW_THRESHOLD:
                        is_distracted = True
                        reasons.append(f"Yaw {yaw:.1f}°")

                    if abs(pitch) > self.PITCH_THRESHOLD:
                        is_distracted = True
                        reasons.append(f"Pitch {pitch:.1f}°")

                    if abs(roll) > self.ROLL_THRESHOLD:
                        is_distracted = True
                        reasons.append(f"Roll {roll:.1f}°")

                x_dev, y_dev, center = self.get_face_center_position(landmarks, frame.shape)
                if x_dev > self.FACE_CENTER_THRESHOLD or y_dev > self.FACE_CENTER_THRESHOLD:
                    is_distracted = True
                    reasons.append("Face Off-Center")

                # Optional: draw mesh
                self.mp_drawing.draw_landmarks(
                    frame,
                    landmarks_obj,
                    self.mp_face_mesh.FACEMESH_CONTOURS,
                    landmark_drawing_spec=None,
                    connection_drawing_spec=self.mp_drawing_styles
                        .get_default_face_mesh_contours_style()
                )

        # Frame tracking
        self.total_frames += 1
        if is_distracted:
            self.consecutive_distracted_frames += 1
        else:
            self.consecutive_distracted_frames = 0

        confirmed = self.consecutive_distracted_frames >= self.DISTRACTION_CONFIRMATION_FRAMES
        if confirmed:
            self.distraction_counter += 1

        return face_detected, confirmed, reasons


def main():
    cap = cv2.VideoCapture(0)
    detector = DistractionDetector()

    print("Press 'q' to quit.")
    while True:
        success, frame = cap.read()
        if not success:
            break

        face_detected, distracted, reasons = detector.detect(frame)

        if not face_detected:
            status = "No Face Detected"
            color = (0, 165, 255)
        elif distracted:
            status = "Distracted"
            color = (0, 0, 255)
        else:
            status = "Focused"
            color = (0, 255, 0)

        cv2.putText(frame, f"Status: {status}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)

        # Reasons (optional)
        y_offset = 60
        for reason in reasons:
            cv2.putText(frame, f"- {reason}", (10, y_offset),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            y_offset += 25

        # Distraction stats
        rate = (detector.distraction_counter / detector.total_frames) * 100 if detector.total_frames else 0
        cv2.putText(frame, f"Distraction Rate: {rate:.1f}%", (10, frame.shape[0] - 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)

        cv2.imshow("Proctoring Detection", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
